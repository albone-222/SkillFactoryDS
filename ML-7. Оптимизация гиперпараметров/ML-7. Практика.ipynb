{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импорт библиотек\n",
    "import numpy as np #для матричных вычислений\n",
    "import pandas as pd #для анализа и предобработки данных\n",
    "import matplotlib.pyplot as plt #для визуализации\n",
    "import seaborn as sns #для визуализации\n",
    "\n",
    "from sklearn import linear_model #линейные моделиё\n",
    "from sklearn import ensemble #ансамбли\n",
    "from sklearn import metrics #метрики\n",
    "from sklearn import model_selection \n",
    "from sklearn.model_selection import train_test_split #сплитование выборки\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497009</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132956</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.273166</td>\n",
       "      <td>0.585445</td>\n",
       "      <td>0.743663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.606291</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111209</td>\n",
       "      <td>0.803455</td>\n",
       "      <td>0.106105</td>\n",
       "      <td>0.411754</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.480124</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209791</td>\n",
       "      <td>0.610350</td>\n",
       "      <td>0.356453</td>\n",
       "      <td>0.517720</td>\n",
       "      <td>0.679051</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538825</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.196344</td>\n",
       "      <td>0.724230</td>\n",
       "      <td>0.235606</td>\n",
       "      <td>0.288764</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.517794</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>0.781422</td>\n",
       "      <td>0.154361</td>\n",
       "      <td>0.303809</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n",
       "1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n",
       "2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n",
       "3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n",
       "4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n",
       "\n",
       "         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n",
       "0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n",
       "1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n",
       "2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n",
       "3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n",
       "4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n",
       "\n",
       "   D1774  D1775  D1776  \n",
       "0      0      0      0  \n",
       "1      0      1      0  \n",
       "2      0      0      0  \n",
       "3      0      0      0  \n",
       "4      0      0      0  \n",
       "\n",
       "[5 rows x 1777 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#считываем данные\n",
    "data = pd.read_csv('data/_train_sem09 (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#осуществляем разбивку на тренировочную и тестовую выборки\n",
    "y = data['Activity']\n",
    "x = data.drop(columns='Activity')\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(x, y, stratify=y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Логистическая регрессия"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартная модель с параметрами по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тестовом наборе: 0.78\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель логистическое регрессии с параметрами по умолчанию\n",
    "log_reg = linear_model.LogisticRegression(random_state=42, max_iter=2000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация логистической регрессии с помощью GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14.2 s\n",
      "Wall time: 10min 42s\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "#задаем сетку параметров\n",
    "params_grid = [\n",
    "    {'penalty': ['l2', 'none'] , \n",
    "    'solver': ['lbfgs', 'sag'], \n",
    "    'C': np.linspace(0.01, 1, 5)}, \n",
    "    \n",
    "    {'penalty': ['l2', 'l1'] , \n",
    "    'solver': ['liblinear', 'saga'], \n",
    "    'C': np.linspace(0.01, 1, 5)}\n",
    "]\n",
    "\n",
    "#инициируем экземпляр GridSearchCV\n",
    "grid_search = model_selection.GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=params_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#обучаем модель и подбираем параметры\n",
    "%time grid_search.fit(X_train, y_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.2575, 'penalty': 'l1', 'solver': 'saga'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#выводим лучшие гиперпараметры модели\n",
    "grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация логистической регрессии с помощью RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.25 s\n",
      "Wall time: 7min 53s\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "#задаем диапазон гиперпараметров\n",
    "params_distrib = [\n",
    "    {'penalty': ['l2', 'none'] , \n",
    "    'solver': ['lbfgs', 'sag', 'newton-cg', 'newton-cholesky'], \n",
    "    'C': np.linspace(0.01, 1, 10)}, \n",
    "    \n",
    "    {'penalty': ['l2', 'l1'] , \n",
    "    'solver': ['liblinear', 'saga'], \n",
    "    'C': np.linspace(0.01, 1, 10)}\n",
    "]\n",
    "\n",
    "#инициируем экземпляр RandomizedSearchCV\n",
    "random_search = model_selection.RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=params_distrib,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    n_iter=50\n",
    ")\n",
    "\n",
    "%time random_search.fit(X_train, y_train)\n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'solver': 'sag', 'penalty': 'l2', 'C': 0.12}\n"
     ]
    }
   ],
   "source": [
    "# выведем гиперпараметры модели\n",
    "print(f'Гиперпараметры модели - {random_search.best_params_}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация логистической регрессии с помощью Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt # импотируем Hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials # и необходимые классы\n",
    "\n",
    "#задаем диапазон гиперпараметров\n",
    "\n",
    "space = hp.choice('variants', [\n",
    "    {\n",
    "        'solver': hp.choice('solver1', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg']),\n",
    "        'C': hp.quniform('C1', 0.01, 1, 0.01),\n",
    "        'penalty': hp.choice('penalty1', ['l2'])\n",
    "    },\n",
    "    {\n",
    "        'solver': hp.choice('solver2', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg']),\n",
    "        'C': hp.quniform('C2', 0.01, 1, 0.01),\n",
    "        'penalty': hp.choice('penalty2', ['none'])\n",
    "    },\n",
    "    {\n",
    "        'solver': hp.choice('solver3', ['liblinear', 'saga']),\n",
    "        'C': hp.quniform('C3', 0.01, 1, 0.01),\n",
    "        'penalty': hp.choice('penalty3', ['l2', 'l1'])\n",
    "    }])\n",
    "\n",
    "# space = hp.choice(\n",
    "#     'variants': [\n",
    "#         {\n",
    "#             'solver': hp.choice('solver1', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg']),\n",
    "#             'C': hp.quniform('C1', 0.01, 1, 0.01),\n",
    "#             'penalty': hp.choice('penalty1', ['l2', 'none'])\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "random_state = 42\n",
    "def hyperopt_lg(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    print(params)\n",
    "    params = {\n",
    "        'solver': str(params['solver']),\n",
    "        'penalty': str(params['penalty']),\n",
    "        'C': round(float(params['C']), 2)\n",
    "    }\n",
    "    \n",
    "    model = linear_model.LogisticRegression(**params, random_state=random_state, max_iter=2000)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    score = model_selection.cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring='f1',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.62, 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
      "{'C': 0.86, 'penalty': 'l2', 'solver': 'liblinear'}                              \n",
      "{'C': 0.08, 'penalty': 'l2', 'solver': 'lbfgs'}                                  \n",
      "{'C': 0.22, 'penalty': 'none', 'solver': 'sag'}                                  \n",
      "  6%|▌         | 3/50 [00:11<02:43,  3.47s/trial, best loss: -0.7909115547558131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.11, 'penalty': 'l2', 'solver': 'newton-cholesky'}                        \n",
      "{'C': 0.45, 'penalty': 'l2', 'solver': 'saga'}                                   \n",
      "{'C': 0.09, 'penalty': 'l2', 'solver': 'saga'}                                   \n",
      "{'C': 0.8300000000000001, 'penalty': 'none', 'solver': 'lbfgs'}                  \n",
      " 14%|█▍        | 7/50 [03:06<19:49, 27.67s/trial, best loss: -0.7909115547558131]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.77, 'penalty': 'l2', 'solver': 'newton-cholesky'}                        \n",
      "{'C': 0.44, 'penalty': 'l2', 'solver': 'liblinear'}                              \n",
      "{'C': 0.05, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.11, 'penalty': 'l2', 'solver': 'saga'}                                    \n",
      "{'C': 0.73, 'penalty': 'none', 'solver': 'sag'}                                   \n",
      " 24%|██▍       | 12/50 [03:49<07:50, 12.37s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}                                \n",
      "{'C': 0.91, 'penalty': 'none', 'solver': 'lbfgs'}                                 \n",
      " 28%|██▊       | 14/50 [06:00<20:10, 33.63s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.99, 'penalty': 'l2', 'solver': 'newton-cg'}                               \n",
      "{'C': 0.8200000000000001, 'penalty': 'l1', 'solver': 'saga'}                      \n",
      " 32%|███▏      | 16/50 [06:24<12:45, 22.51s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.11, 'penalty': 'none', 'solver': 'sag'}                                   \n",
      " 34%|███▍      | 17/50 [09:41<41:05, 74.71s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.14, 'penalty': 'l2', 'solver': 'liblinear'}                               \n",
      "{'C': 0.72, 'penalty': 'l2', 'solver': 'sag'}                                     \n",
      "{'C': 0.04, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.3, 'penalty': 'l2', 'solver': 'lbfgs'}                                    \n",
      "{'C': 0.3, 'penalty': 'l2', 'solver': 'lbfgs'}                                    \n",
      "{'C': 0.26, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.02, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.44, 'penalty': 'l2', 'solver': 'sag'}                                     \n",
      "{'C': 0.02, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.18, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.43, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.19, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}                                     \n",
      "{'C': 0.9500000000000001, 'penalty': 'l2', 'solver': 'newton-cg'}                 \n",
      "{'C': 0.39, 'penalty': 'l2', 'solver': 'newton-cholesky'}                         \n",
      "{'C': 0.5, 'penalty': 'none', 'solver': 'newton-cg'}                              \n",
      " 66%|██████▌   | 33/50 [13:30<01:51,  6.53s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.56, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.14, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.43, 'penalty': 'none', 'solver': 'newton-cholesky'}                       \n",
      " 72%|███████▏  | 36/50 [14:38<03:04, 13.15s/trial, best loss: -0.7914103753551599]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_glm\\_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.57158e-23): result may not be accurate.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.23, 'penalty': 'l2', 'solver': 'lbfgs'}                                   \n",
      "{'C': 0.07, 'penalty': 'l2', 'solver': 'newton-cholesky'}                         \n",
      "{'C': 0.35000000000000003, 'penalty': 'l2', 'solver': 'newton-cholesky'}          \n",
      "{'C': 0.6900000000000001, 'penalty': 'l1', 'solver': 'liblinear'}                 \n",
      "{'C': 0.04, 'penalty': 'none', 'solver': 'newton-cg'}                             \n",
      " 82%|████████▏ | 41/50 [15:06<00:51,  5.73s/trial, best loss: -0.7919953322184076]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.12, 'penalty': 'l2', 'solver': 'newton-cholesky'}                         \n",
      "{'C': 0.63, 'penalty': 'l1', 'solver': 'saga'}                                    \n",
      " 86%|████████▌ | 43/50 [16:08<01:53, 16.25s/trial, best loss: -0.7919953322184076]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.29, 'penalty': 'none', 'solver': 'newton-cholesky'}                       \n",
      " 88%|████████▊ | 44/50 [19:22<06:57, 69.52s/trial, best loss: -0.7919953322184076]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_glm\\_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.57158e-23): result may not be accurate.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.5, 'penalty': 'l2', 'solver': 'newton-cholesky'}                          \n",
      "{'C': 0.99, 'penalty': 'l1', 'solver': 'saga'}                                    \n",
      " 92%|█████████▏| 46/50 [19:40<02:33, 38.27s/trial, best loss: -0.7919953322184076]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.88, 'penalty': 'l2', 'solver': 'newton-cholesky'}                         \n",
      "{'C': 0.68, 'penalty': 'none', 'solver': 'lbfgs'}                                 \n",
      " 96%|█████████▌| 48/50 [23:00<02:02, 61.07s/trial, best loss: -0.7919953322184076]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.32, 'penalty': 'l1', 'solver': 'liblinear'}                               \n",
      "100%|██████████| 50/50 [23:16<00:00, 27.93s/trial, best loss: -0.7919953322184076]\n",
      "Наилучшие значения гиперпараметров - {'C1': 0.07, 'penalty1': 0, 'solver1': 2, 'variants': 0}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_params = fmin(\n",
    "    hyperopt_lg,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=50,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print(f'Наилучшие значения гиперпараметров - {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'C': 0.07, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "params_opt = hyperopt.space_eval(space, best_params)\n",
    "\n",
    "log_reg = linear_model.LogisticRegression(**params_opt, random_state=random_state, max_iter=2000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print(f'Гиперпараметры модели - {params_opt}')\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация логистической регрессии с помощью Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "random_state = 42\n",
    "def optuna_log_reg(trial):\n",
    "    # var1 = {\n",
    "    #     'solver': trial.suggest_categorical('solver', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg']),\n",
    "    #     'C': trial.suggest_float('C', 0.01, 1),\n",
    "    #     'penalty': trial.suggest_categorical('penalty', ['l2'])\n",
    "    # }\n",
    "    # var2 = {\n",
    "    #     'solver': trial.suggest_categorical('solver', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg']),\n",
    "    #     'C': trial.suggest_float('C', 0.01, 1),\n",
    "    #     'penalty': trial.suggest_categorical('penalty', ['none'])\n",
    "    # }\n",
    "    # var3 = {\n",
    "    #     'solver': trial.suggest_categorical('solver', ['liblinear', 'saga']),\n",
    "    #     'C': trial.suggest_float('C', 0.01, 1),\n",
    "    #     'penalty': trial.suggest_categorical('penalty', ['l2', 'l1'])\n",
    "    # }\n",
    "    \n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'sag', 'newton-cholesky', 'newton-cg'])\n",
    "    penalty = trial.suggest_categorical('penalty', ['l2', 'none'])\n",
    "    C = trial.suggest_float('C', 0.01, 1)\n",
    "    #variants = trial.suggest_categorical('variants', [var1, var2, var3])\n",
    "    \n",
    "    model = linear_model.LogisticRegression(solver=solver, \n",
    "                                            random_state=random_state, \n",
    "                                            max_iter=2000,\n",
    "                                            C=C,\n",
    "                                            penalty=penalty)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    score = model_selection.cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        scoring='f1',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-16 18:43:10,013]\u001b[0m A new study created in memory with name: LogisticRegression\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:45:15,576]\u001b[0m Trial 0 finished with value: 0.742379894257529 and parameters: {'solver': 'sag', 'penalty': 'none', 'C': 0.8480378439637759}. Best is trial 0 with value: 0.742379894257529.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:45:24,564]\u001b[0m Trial 1 finished with value: 0.7728364550081914 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.6885638749108446}. Best is trial 1 with value: 0.7728364550081914.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:45:29,110]\u001b[0m Trial 2 finished with value: 0.7802724328192985 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.24962337935194004}. Best is trial 2 with value: 0.7802724328192985.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:45:33,740]\u001b[0m Trial 3 finished with value: 0.7757655074784271 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.48642865338057645}. Best is trial 2 with value: 0.7802724328192985.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:45:41,628]\u001b[0m Trial 4 finished with value: 0.7743121889030238 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.6248971700869899}. Best is trial 2 with value: 0.7802724328192985.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:46:01,749]\u001b[0m Trial 5 finished with value: 0.7877502045672051 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.12643753500518776}. Best is trial 5 with value: 0.7877502045672051.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:46:06,120]\u001b[0m Trial 6 finished with value: 0.791001807887769 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.04959192151573118}. Best is trial 6 with value: 0.791001807887769.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:46:12,623]\u001b[0m Trial 7 finished with value: 0.775318114305318 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.5801769840485977}. Best is trial 6 with value: 0.791001807887769.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:48:19,364]\u001b[0m Trial 8 finished with value: 0.742379894257529 and parameters: {'solver': 'sag', 'penalty': 'none', 'C': 0.8737006190118801}. Best is trial 6 with value: 0.791001807887769.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:48:34,453]\u001b[0m Trial 9 finished with value: 0.7211700273026267 and parameters: {'solver': 'lbfgs', 'penalty': 'none', 'C': 0.6763837158030017}. Best is trial 6 with value: 0.791001807887769.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:48:37,887]\u001b[0m Trial 10 finished with value: 0.7843148384742002 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.010728061604996654}. Best is trial 6 with value: 0.791001807887769.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:48:48,459]\u001b[0m Trial 11 finished with value: 0.7919578995306507 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.03331437075237584}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:48:55,043]\u001b[0m Trial 12 finished with value: 0.7805406979982408 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.26311060483213006}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:49:01,051]\u001b[0m Trial 13 finished with value: 0.7858548930999245 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.011957081089674762}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:49:25,524]\u001b[0m Trial 14 finished with value: 0.780271546604247 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.24491718687345193}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "\u001b[32m[I 2023-05-16 18:50:21,433]\u001b[0m Trial 15 finished with value: 0.705948396585675 and parameters: {'solver': 'newton-cg', 'penalty': 'none', 'C': 0.43448609655131704}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:50:25,246]\u001b[0m Trial 16 finished with value: 0.7890724716343359 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.11091775159584195}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:50:30,510]\u001b[0m Trial 17 finished with value: 0.7800807935810385 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.3509625897579426}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:52:39,073]\u001b[0m Trial 18 finished with value: 0.742379894257529 and parameters: {'solver': 'sag', 'penalty': 'none', 'C': 0.1760448167003878}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:52:46,159]\u001b[0m Trial 19 finished with value: 0.7799196118390155 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.36475912875625377}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:52:51,206]\u001b[0m Trial 20 finished with value: 0.7909098809560493 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.08473238905479683}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:52:56,319]\u001b[0m Trial 21 finished with value: 0.7909115547558131 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.08004886648860848}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:00,419]\u001b[0m Trial 22 finished with value: 0.7897051647076967 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.018184688014110807}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:06,412]\u001b[0m Trial 23 finished with value: 0.7834453526290253 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.17611963723451657}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:11,434]\u001b[0m Trial 24 finished with value: 0.7913255231525135 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.08245818593335658}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:34,111]\u001b[0m Trial 25 finished with value: 0.7856332900704182 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.17012307855598696}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:53:49,487]\u001b[0m Trial 26 finished with value: 0.7211700273026267 and parameters: {'solver': 'lbfgs', 'penalty': 'none', 'C': 0.0654684367133363}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:53,314]\u001b[0m Trial 27 finished with value: 0.7868142716841032 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.14834946888550254}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:53:59,759]\u001b[0m Trial 28 finished with value: 0.7809994269276026 and parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'C': 0.22637903647593244}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:56:06,067]\u001b[0m Trial 29 finished with value: 0.742379894257529 and parameters: {'solver': 'sag', 'penalty': 'none', 'C': 0.3155272191497652}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:56:22,905]\u001b[0m Trial 30 finished with value: 0.7914484475825634 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.08392365577122991}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:56:38,872]\u001b[0m Trial 31 finished with value: 0.7907523365229708 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.07568651542589586}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:56:59,572]\u001b[0m Trial 32 finished with value: 0.7868002976607482 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.13878833848471495}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:57:13,446]\u001b[0m Trial 33 finished with value: 0.7914442637165282 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.054874534371661204}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:57:36,859]\u001b[0m Trial 34 finished with value: 0.7801980742933667 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.20206882754991895}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:57:56,385]\u001b[0m Trial 35 finished with value: 0.7884049195755741 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.11798756824080169}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:58:20,858]\u001b[0m Trial 36 finished with value: 0.7701462680088846 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.9439557943892415}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:58:45,938]\u001b[0m Trial 37 finished with value: 0.7802786277108754 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.27485426732704166}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:09,486]\u001b[0m Trial 38 finished with value: 0.7814630452002269 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.1988335984191619}. Best is trial 11 with value: 0.7919578995306507.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:13,517]\u001b[0m Trial 39 finished with value: 0.7921316095996123 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.06304413681525046}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "d:\\Program Files\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_glm\\_newton_solver.py:491: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.57158e-23): result may not be accurate.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-05-16 18:59:26,232]\u001b[0m Trial 40 finished with value: 0.7150692275945185 and parameters: {'solver': 'newton-cholesky', 'penalty': 'none', 'C': 0.04933323105971587}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:30,144]\u001b[0m Trial 41 finished with value: 0.7882626350278198 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.11450511405748273}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:34,121]\u001b[0m Trial 42 finished with value: 0.7904631972876551 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.05302870770746085}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:37,959]\u001b[0m Trial 43 finished with value: 0.7868142716841032 and parameters: {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.14409609906460247}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:45,223]\u001b[0m Trial 44 finished with value: 0.7917682285169585 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.01664593957335258}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 18:59:51,202]\u001b[0m Trial 45 finished with value: 0.7855840608523208 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.011606137688504818}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 19:00:09,979]\u001b[0m Trial 46 finished with value: 0.7896325365313084 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.10803714769835449}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 19:00:22,483]\u001b[0m Trial 47 finished with value: 0.7915490253163262 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.044620171510570385}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 19:00:28,432]\u001b[0m Trial 48 finished with value: 0.785852587092776 and parameters: {'solver': 'sag', 'penalty': 'l2', 'C': 0.011369924186430869}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 19:00:32,941]\u001b[0m Trial 49 finished with value: 0.7809994269276026 and parameters: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.22540074444437525}. Best is trial 39 with value: 0.7921316095996123.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 43s\n",
      "Wall time: 17min 22s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study = optuna.create_study(study_name=\"LogisticRegression\", direction=\"maximize\")\n",
    "%time study.optimize(optuna_log_reg, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.06304413681525046}\n",
      "f1_score на обучающем наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'solver': 'newton-cholesky', 'penalty': 'l2', 'C': 0.06304413681525046}\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "log_reg = linear_model.LogisticRegression(**study.best_params, \n",
    "                                        random_state=random_state, \n",
    "                                        max_iter=2000,\n",
    "                                        )\n",
    "    \n",
    "log_reg.fit(X_train, y_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "print(f'Гиперпараметры модели - {study.best_params}')\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Случайный лес"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартная модель с параметрами по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score на тренировочном наборе: 1.00\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "# обучаем модель случайного леса с параметрами по умолчанию\n",
    "model_rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_test_pred = model_rf.predict(X_test)\n",
    "print(f'f1_score на тренировочном наборе: {metrics.f1_score(y_train, model_rf.predict(X_train)):.2f}')\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация логистической регрессии с помощью GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.09 s\n",
      "Wall time: 45.9 s\n",
      "f1_score на тестовом наборе: 0.80\n"
     ]
    }
   ],
   "source": [
    "#задаем сетку параметров\n",
    "params_grid = {\n",
    "    'criterion': ['entropy','gini'], \n",
    "    'max_depth': np.arange(10, 300, 60), \n",
    "    'min_samples_leaf': np.arange(3, 30, 6)\n",
    "    }\n",
    "    \n",
    "#инициируем экземпляр GridSearchCV\n",
    "grid_search = model_selection.GridSearchCV(\n",
    "    estimator=model_rf,\n",
    "    param_grid=params_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#обучаем модель и подбираем параметры\n",
    "%time grid_search.fit(X_train, y_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 70, 'min_samples_leaf': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#выводим лучшие гиперпараметры модели\n",
    "grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация модели случайного леса с помощью RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 891 ms\n",
      "Wall time: 39.3 s\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "#задаем диапазон гиперпараметров\n",
    "params_distrib = {\n",
    "    'criterion': ['entropy','gini'], \n",
    "    'max_depth': np.arange(10, 300, 50), \n",
    "    'min_samples_leaf': np.arange(3, 30, 1)\n",
    "    }\n",
    "\n",
    "#инициируем экземпляр RandomizedSearchCV\n",
    "random_search = model_selection.RandomizedSearchCV(\n",
    "    estimator=model_rf,\n",
    "    param_distributions=params_distrib,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    n_iter=50\n",
    ")\n",
    "\n",
    "%time random_search.fit(X_train, y_train)\n",
    "y_test_pred = random_search.predict(X_test)\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'min_samples_leaf': 3, 'max_depth': 110, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "# выведем гиперпараметры модели\n",
    "print(f'Гиперпараметры модели - {random_search.best_params_}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация модели случайного леса с помощью Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt # импотируем Hyperopt\n",
    "from hyperopt import hp, fmin, tpe, Trials # и необходимые классы\n",
    "\n",
    "#задаем диапазон гиперпараметров\n",
    "\n",
    "space = {\n",
    "        'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 300, 1),\n",
    "        'min_samples_leaf': hp.quniform('min_samples_leaf', 3, 10, 1)\n",
    "    }\n",
    "    \n",
    "random_state = 42\n",
    "def hyperopt_rf(params, cv=5, X=X_train, y=y_train, random_state=random_state):\n",
    "    print(params)\n",
    "    params = {\n",
    "        'criterion': str(params['criterion']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_leaf': int(params['min_samples_leaf'])\n",
    "    }\n",
    "    \n",
    "    model = ensemble.RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    score = model_selection.cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring='f1',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 190.0, 'min_samples_leaf': 9.0}\n",
      "{'criterion': 'entropy', 'max_depth': 186.0, 'min_samples_leaf': 5.0}            \n",
      "{'criterion': 'gini', 'max_depth': 31.0, 'min_samples_leaf': 4.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 72.0, 'min_samples_leaf': 6.0}             \n",
      "{'criterion': 'gini', 'max_depth': 38.0, 'min_samples_leaf': 7.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 57.0, 'min_samples_leaf': 8.0}             \n",
      "{'criterion': 'gini', 'max_depth': 299.0, 'min_samples_leaf': 8.0}               \n",
      "{'criterion': 'gini', 'max_depth': 251.0, 'min_samples_leaf': 3.0}               \n",
      "{'criterion': 'gini', 'max_depth': 234.0, 'min_samples_leaf': 8.0}               \n",
      "{'criterion': 'entropy', 'max_depth': 149.0, 'min_samples_leaf': 5.0}            \n",
      "{'criterion': 'entropy', 'max_depth': 22.0, 'min_samples_leaf': 3.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 186.0, 'min_samples_leaf': 8.0}             \n",
      "{'criterion': 'gini', 'max_depth': 222.0, 'min_samples_leaf': 5.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 145.0, 'min_samples_leaf': 3.0}             \n",
      "{'criterion': 'gini', 'max_depth': 273.0, 'min_samples_leaf': 5.0}                \n",
      "{'criterion': 'gini', 'max_depth': 296.0, 'min_samples_leaf': 9.0}                \n",
      "{'criterion': 'gini', 'max_depth': 246.0, 'min_samples_leaf': 9.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 40.0, 'min_samples_leaf': 5.0}              \n",
      "{'criterion': 'gini', 'max_depth': 256.0, 'min_samples_leaf': 5.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 217.0, 'min_samples_leaf': 6.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 95.0, 'min_samples_leaf': 6.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 93.0, 'min_samples_leaf': 6.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 135.0, 'min_samples_leaf': 7.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 100.0, 'min_samples_leaf': 6.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 106.0, 'min_samples_leaf': 7.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 73.0, 'min_samples_leaf': 4.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 122.0, 'min_samples_leaf': 4.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 94.0, 'min_samples_leaf': 6.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 169.0, 'min_samples_leaf': 10.0}            \n",
      "{'criterion': 'entropy', 'max_depth': 115.0, 'min_samples_leaf': 7.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 215.0, 'min_samples_leaf': 6.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 206.0, 'min_samples_leaf': 4.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 169.0, 'min_samples_leaf': 7.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 72.0, 'min_samples_leaf': 6.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 70.0, 'min_samples_leaf': 10.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 282.0, 'min_samples_leaf': 7.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 15.0, 'min_samples_leaf': 5.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 52.0, 'min_samples_leaf': 4.0}              \n",
      "{'criterion': 'entropy', 'max_depth': 127.0, 'min_samples_leaf': 8.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 191.0, 'min_samples_leaf': 6.0}             \n",
      "{'criterion': 'gini', 'max_depth': 170.0, 'min_samples_leaf': 7.0}                \n",
      "{'criterion': 'entropy', 'max_depth': 202.0, 'min_samples_leaf': 8.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 237.0, 'min_samples_leaf': 5.0}             \n",
      "{'criterion': 'gini', 'max_depth': 97.0, 'min_samples_leaf': 4.0}                 \n",
      "{'criterion': 'entropy', 'max_depth': 159.0, 'min_samples_leaf': 8.0}             \n",
      "{'criterion': 'entropy', 'max_depth': 86.0, 'min_samples_leaf': 3.0}              \n",
      "{'criterion': 'gini', 'max_depth': 31.0, 'min_samples_leaf': 5.0}                 \n",
      "{'criterion': 'entropy', 'max_depth': 143.0, 'min_samples_leaf': 9.0}             \n",
      "{'criterion': 'gini', 'max_depth': 55.0, 'min_samples_leaf': 6.0}                 \n",
      "{'criterion': 'entropy', 'max_depth': 259.0, 'min_samples_leaf': 5.0}             \n",
      "100%|██████████| 50/50 [02:19<00:00,  2.79s/trial, best loss: -0.8191429143451181]\n",
      "Наилучшие значения гиперпараметров - {'criterion': 1, 'max_depth': 72.0, 'min_samples_leaf': 6.0}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_params = fmin(\n",
    "    hyperopt_rf,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=50,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print(f'Наилучшие значения гиперпараметров - {best_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'criterion': 'entropy', 'max_depth': 72.0, 'min_samples_leaf': 6.0}\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "params_opt = hyperopt.space_eval(space, best_params)\n",
    "max_depth = int(params_opt['max_depth'])\n",
    "min_samples_leaf = int(params_opt['min_samples_leaf'])\n",
    "\n",
    "model_rf = ensemble.RandomForestClassifier(max_depth=max_depth, \n",
    "                                           random_state=random_state,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           criterion=params_opt['criterion'])\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred = model_rf.predict(X_test)\n",
    "print(f'Гиперпараметры модели - {params_opt}')\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация модели случайного леса с помощью Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "random_state = 42\n",
    "def optuna_random_forest(trial):\n",
    "    \n",
    "    criterion = trial.suggest_categorical('criterion', ['entropy', 'gini'])\n",
    "    max_depth = trial.suggest_int('max_depth', 10, 300)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 3, 10)\n",
    "        \n",
    "    model = ensemble.RandomForestClassifier(max_depth=max_depth, \n",
    "                                            random_state=random_state, \n",
    "                                            criterion=criterion,\n",
    "                                            min_samples_leaf=min_samples_leaf)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    score = model_selection.cross_val_score(\n",
    "        estimator=model,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        scoring='f1',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "    \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-16 20:05:35,216]\u001b[0m A new study created in memory with name: RandomForestClassifier\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:39,280]\u001b[0m Trial 0 finished with value: 0.8158904004335706 and parameters: {'criterion': 'entropy', 'max_depth': 13, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:42,381]\u001b[0m Trial 1 finished with value: 0.8091485172524348 and parameters: {'criterion': 'gini', 'max_depth': 80, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:44,795]\u001b[0m Trial 2 finished with value: 0.8052070176293349 and parameters: {'criterion': 'entropy', 'max_depth': 43, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:47,033]\u001b[0m Trial 3 finished with value: 0.805291739257487 and parameters: {'criterion': 'gini', 'max_depth': 129, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:49,474]\u001b[0m Trial 4 finished with value: 0.8052070176293349 and parameters: {'criterion': 'entropy', 'max_depth': 259, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:52,488]\u001b[0m Trial 5 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 188, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:55,536]\u001b[0m Trial 6 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 77, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:05:58,475]\u001b[0m Trial 7 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 160, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8158904004335706.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:01,658]\u001b[0m Trial 8 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 247, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:04,352]\u001b[0m Trial 9 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 96, 'min_samples_leaf': 7}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:07,121]\u001b[0m Trial 10 finished with value: 0.8166351659194063 and parameters: {'criterion': 'gini', 'max_depth': 296, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:09,861]\u001b[0m Trial 11 finished with value: 0.8166351659194063 and parameters: {'criterion': 'gini', 'max_depth': 299, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:12,314]\u001b[0m Trial 12 finished with value: 0.8090809009537571 and parameters: {'criterion': 'gini', 'max_depth': 242, 'min_samples_leaf': 6}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:15,065]\u001b[0m Trial 13 finished with value: 0.8166351659194063 and parameters: {'criterion': 'gini', 'max_depth': 300, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:17,652]\u001b[0m Trial 14 finished with value: 0.8125964796971756 and parameters: {'criterion': 'gini', 'max_depth': 216, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:19,977]\u001b[0m Trial 15 finished with value: 0.8099127740343459 and parameters: {'criterion': 'gini', 'max_depth': 260, 'min_samples_leaf': 8}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:23,016]\u001b[0m Trial 16 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 215, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:25,778]\u001b[0m Trial 17 finished with value: 0.8135706361736549 and parameters: {'criterion': 'entropy', 'max_depth': 202, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:28,646]\u001b[0m Trial 18 finished with value: 0.8135706361736549 and parameters: {'criterion': 'entropy', 'max_depth': 169, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:31,296]\u001b[0m Trial 19 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 231, 'min_samples_leaf': 7}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:34,165]\u001b[0m Trial 20 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 131, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:36,944]\u001b[0m Trial 21 finished with value: 0.8166351659194063 and parameters: {'criterion': 'gini', 'max_depth': 276, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:40,069]\u001b[0m Trial 22 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 234, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:43,089]\u001b[0m Trial 23 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 223, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:46,060]\u001b[0m Trial 24 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 250, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:48,863]\u001b[0m Trial 25 finished with value: 0.8135706361736549 and parameters: {'criterion': 'entropy', 'max_depth': 204, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:51,986]\u001b[0m Trial 26 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 180, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:54,833]\u001b[0m Trial 27 finished with value: 0.8139730200694144 and parameters: {'criterion': 'entropy', 'max_depth': 141, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:06:58,016]\u001b[0m Trial 28 finished with value: 0.8172282228430638 and parameters: {'criterion': 'entropy', 'max_depth': 273, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.8172282228430638.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:00,670]\u001b[0m Trial 29 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 235, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:03,347]\u001b[0m Trial 30 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 200, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:06,137]\u001b[0m Trial 31 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 200, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:08,804]\u001b[0m Trial 32 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 195, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:11,556]\u001b[0m Trial 33 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 193, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:14,282]\u001b[0m Trial 34 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 142, 'min_samples_leaf': 7}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:16,939]\u001b[0m Trial 35 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 174, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:19,495]\u001b[0m Trial 36 finished with value: 0.8118311427291707 and parameters: {'criterion': 'entropy', 'max_depth': 197, 'min_samples_leaf': 8}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:22,259]\u001b[0m Trial 37 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 160, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:24,808]\u001b[0m Trial 38 finished with value: 0.8118311427291707 and parameters: {'criterion': 'entropy', 'max_depth': 220, 'min_samples_leaf': 8}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:27,545]\u001b[0m Trial 39 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 106, 'min_samples_leaf': 7}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:30,269]\u001b[0m Trial 40 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 40, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:32,962]\u001b[0m Trial 41 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 189, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:35,719]\u001b[0m Trial 42 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 207, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:38,538]\u001b[0m Trial 43 finished with value: 0.8135706361736549 and parameters: {'criterion': 'entropy', 'max_depth': 187, 'min_samples_leaf': 5}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:41,113]\u001b[0m Trial 44 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 157, 'min_samples_leaf': 7}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:43,831]\u001b[0m Trial 45 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 193, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:46,601]\u001b[0m Trial 46 finished with value: 0.8135706361736549 and parameters: {'criterion': 'entropy', 'max_depth': 233, 'min_samples_leaf': 5}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:49,210]\u001b[0m Trial 47 finished with value: 0.8129309790386238 and parameters: {'criterion': 'entropy', 'max_depth': 172, 'min_samples_leaf': 7}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:51,938]\u001b[0m Trial 48 finished with value: 0.8191429143451181 and parameters: {'criterion': 'entropy', 'max_depth': 260, 'min_samples_leaf': 6}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n",
      "\u001b[32m[I 2023-05-16 20:07:54,369]\u001b[0m Trial 49 finished with value: 0.8091485172524348 and parameters: {'criterion': 'gini', 'max_depth': 216, 'min_samples_leaf': 9}. Best is trial 29 with value: 0.8191429143451181.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15 s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"RandomForestClassifier\", direction=\"maximize\")\n",
    "%time study.optimize(optuna_random_forest, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'criterion': 'entropy', 'max_depth': 235, 'min_samples_leaf': 6}\n",
      "f1_score на обучающем наборе: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гиперпараметры модели - {'criterion': 'entropy', 'max_depth': 235, 'min_samples_leaf': 6}\n",
      "f1_score на тестовом наборе: 0.79\n"
     ]
    }
   ],
   "source": [
    "model_rf = ensemble.RandomForestClassifier(**study.best_params, \n",
    "                                        random_state=random_state)\n",
    "    \n",
    "model_rf.fit(X_train, y_train)\n",
    "y_test_pred = model_rf.predict(X_test)\n",
    "print(f'Гиперпараметры модели - {study.best_params}')\n",
    "print(f'f1_score на тестовом наборе: {metrics.f1_score(y_test, y_test_pred):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
